{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12492646,"sourceType":"datasetVersion","datasetId":7883632},{"sourceId":12500893,"sourceType":"datasetVersion","datasetId":7888566},{"sourceId":12634931,"sourceType":"datasetVersion","datasetId":7983944}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom datasets import DatasetDict, Dataset\nfrom PIL import Image\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments, pipeline\nimport evaluate\nimport gc\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T16:26:48.187911Z","iopub.execute_input":"2025-08-01T16:26:48.188623Z","iopub.status.idle":"2025-08-01T16:27:07.916511Z","shell.execute_reply.started":"2025-08-01T16:26:48.188593Z","shell.execute_reply":"2025-08-01T16:27:07.915930Z"}},"outputs":[{"name":"stderr","text":"2025-08-01 16:26:59.186553: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754065619.441425    3663 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754065619.509008    3663 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Configuración de rutas\nbase_path = \"/kaggle/input/nombre-column/rows_Nombre/rows_Nombre/rows_Nombre - copia\"\n\n# 1. Recopilar todos los datos\ndata = []\nfor folder_num in range(4, 30):  # Carpetas del 4 al 29\n    folder_path = os.path.join(base_path, str(folder_num))\n    \n    if not os.path.exists(folder_path):\n        print(f\"Advertencia: No existe la carpeta {folder_path}\")\n        continue\n    \n    # Procesar cada imagen en la carpeta\n    for img_file in sorted(os.listdir(folder_path)):\n        if img_file.endswith('.png'):\n            img_path = os.path.join(folder_path, img_file)\n            label = os.path.splitext(img_file)[0]  # Elimina .png\n            \n            data.append({\n                'folder': folder_num,\n                'image_path': img_path,\n                'label': label\n            })\n\n# 2. Crear DataFrame\ndf = pd.DataFrame(data)\n\n# Verificar que las imágenes existen\ndf['exists'] = df['image_path'].apply(os.path.exists)\nprint(f\"Imágenes encontradas: {df['exists'].sum()}/{len(df)}\")\ndf = df[df['exists']].drop(columns=['exists'])\n\n# 3. Dividir en train/test (estratificado por carpeta)\ntrain_df, test_df = train_test_split(\n    df, \n    test_size=0.2, \n    random_state=42,\n    stratify=df['folder']  # Mantener proporción por carpeta\n)\n\n# 4. Crear datasets HuggingFace\ndef create_dataset(dataframe):\n    return Dataset.from_dict({\n        'image': [Image.open(path).convert('RGB') for path in dataframe['image_path']],\n        'text': list(dataframe['label'])\n    })\n\ndataset = DatasetDict({\n    'train': create_dataset(train_df),\n    'test': create_dataset(test_df)\n})\n\n# 5. Verificación\nprint(\"\\nResumen del dataset:\")\nprint(f\"Entrenamiento: {len(dataset['train'])} ejemplos\")\nprint(f\"Prueba: {len(dataset['test'])} ejemplos\")\nprint(\"\\nEjemplo del primer elemento:\")\nprint(dataset['train'][0]['text'])  # Debería mostrar el nombre del archivo sin .png","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T16:27:07.917679Z","iopub.execute_input":"2025-08-01T16:27:07.918325Z","iopub.status.idle":"2025-08-01T16:27:17.599068Z","shell.execute_reply.started":"2025-08-01T16:27:07.918302Z","shell.execute_reply":"2025-08-01T16:27:17.598057Z"}},"outputs":[{"name":"stdout","text":"Imágenes encontradas: 1300/1300\n\nResumen del dataset:\nEntrenamiento: 1040 ejemplos\nPrueba: 260 ejemplos\n\nEjemplo del primer elemento:\nRamona d. De Colombo Siani\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\ndef prepare_examples(batch):\n    images = batch['image']\n    texts = batch['text']\n    \n    # Procesar imágenes\n    pixel_values = processor(images, return_tensors=\"pt\").pixel_values\n    \n    # Procesar textos\n    labels = processor.tokenizer(texts, padding=\"max_length\", max_length=64).input_ids\n    \n    # Reemplazar padding token id por -100 para ignorar en loss\n    labels = [[label if label != processor.tokenizer.pad_token_id else -100 for label in labels_example] for labels_example in labels]\n    \n    batch['pixel_values'] = pixel_values\n    batch['labels'] = labels\n    \n    return batch\n\n# Aplicar el preprocesamiento\ntrain_dataset = dataset['train'].map(prepare_examples, batched=True, batch_size=8)\neval_dataset = dataset['test'].map(prepare_examples, batched=True, batch_size=8)\n\n# Configurar formato para PyTorch\ntrain_dataset.set_format(type='torch', columns=['pixel_values', 'labels'])\neval_dataset.set_format(type='torch', columns=['pixel_values', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T16:27:17.599868Z","iopub.execute_input":"2025-08-01T16:27:17.600124Z","iopub.status.idle":"2025-08-01T16:27:43.919006Z","shell.execute_reply.started":"2025-08-01T16:27:17.600102Z","shell.execute_reply":"2025-08-01T16:27:43.918089Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1040 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da87b6d378794a8ea89c60552a82828d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/260 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e7787d007e3443f8cece9580638e88e"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n\n# Configurar parámetros del modelo\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# Configuración para generación\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T16:27:43.921157Z","iopub.execute_input":"2025-08-01T16:27:43.921417Z","iopub.status.idle":"2025-08-01T16:27:47.820034Z","shell.execute_reply.started":"2025-08-01T16:27:43.921394Z","shell.execute_reply":"2025-08-01T16:27:47.819173Z"}},"outputs":[{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"cer_metric = evaluate.load('cer')\n \ndef compute_cer(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n \n \n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n \n \n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n \n    return {\"cer\": cer}\n\ntraining_args = Seq2SeqTrainingArguments(\noutput_dir=\"./trocr-trained\",\nper_device_train_batch_size=8,  # Aumentado\nper_device_eval_batch_size=8,\nnum_train_epochs=15,\nfp16=True,\nlearning_rate=5e-5,\ngradient_accumulation_steps=2,  # Útil si hay límite de GPU\nlr_scheduler_type=\"cosine\",  # Mejor convergencia\nwarmup_ratio=0.1,\nweight_decay=0.01,\neval_strategy=\"epoch\",\neval_steps=100,\nlogging_strategy=\"epoch\",\nlogging_steps=50,\nsave_strategy=\"epoch\",\nsave_total_limit=3,\nload_best_model_at_end=True,\nmetric_for_best_model=\"cer\",\ngreater_is_better=False,\npredict_with_generate=True,\nreport_to=\"none\",\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    processing_class=processor.image_processor,\n    args=training_args,\n    compute_metrics=compute_cer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T16:28:31.819499Z","iopub.execute_input":"2025-08-01T16:28:31.820296Z","iopub.status.idle":"2025-08-01T16:28:38.376316Z","shell.execute_reply.started":"2025-08-01T16:28:31.820261Z","shell.execute_reply":"2025-08-01T16:28:38.375697Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nres = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T16:28:42.138561Z","iopub.execute_input":"2025-08-01T16:28:42.138909Z"}},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='71' max='495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 71/495 07:09 < 43:57, 0.16 it/s, Epoch 2.12/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Cer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.846700</td>\n      <td>5.042746</td>\n      <td>0.626640</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.946800</td>\n      <td>4.518231</td>\n      <td>0.567962</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1730: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Guardar el modelo entrenado\ntrainer.save_model(\"trocr-trained-custom\")\nprocessor.save_pretrained(\"trocr-trained-custom\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T16:27:48.324296Z","iopub.status.idle":"2025-08-01T16:27:48.324631Z","shell.execute_reply.started":"2025-08-01T16:27:48.324457Z","shell.execute_reply":"2025-08-01T16:27:48.324474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_dir = \"/kaggle/input/image-test/\"\n\n# Cargar el modelo entrenado\nocr = pipeline(\"image-to-text\", model=\"./trocr-trained-custom\")\n\n# Probar con una imagen de test\ntest_image = Image.open(os.path.join(images_dir, \"image_test_3.png\")).convert(\"RGB\")\nprediction = ocr(test_image)\nprint(f\"Predicción: {prediction[0]['generated_text']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T16:27:48.325591Z","iopub.status.idle":"2025-08-01T16:27:48.325892Z","shell.execute_reply.started":"2025-08-01T16:27:48.325769Z","shell.execute_reply":"2025-08-01T16:27:48.325780Z"}},"outputs":[],"execution_count":null}]}
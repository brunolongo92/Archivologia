{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12492646,"sourceType":"datasetVersion","datasetId":7883632},{"sourceId":12500893,"sourceType":"datasetVersion","datasetId":7888566}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \\\n    transformers==4.34.0 \\\n    datasets==2.14.6 \\\n    evaluate==0.4.1 \\\n    pytorch-lightning==2.0.9 \\\n    torch==2.0.1 \\\n    torchvision==0.15.2 \\\n    jiwer==3.0.3 \\\n    pandas==2.1.0 \\\n    numpy==1.24.4 \\\n    -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:48:35.363525Z","iopub.execute_input":"2025-07-17T16:48:35.363799Z","iopub.status.idle":"2025-07-17T16:49:55.272074Z","shell.execute_reply.started":"2025-07-17T16:48:35.363773Z","shell.execute_reply":"2025-07-17T16:49:55.271440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cargar el CSV\ncsv_path = \"/kaggle/input/hwr-test/output_labels.csv\"\nimages_dir = \"/kaggle/input/hwr-test/output_lines\"\n\n# Leer el CSV\ndf = pd.read_csv(csv_path)\n\n# Verificar que las imágenes existen\ndef check_image_exists(row):\n    return os.path.exists(os.path.join(images_dir, row['filename']))\n\ndf = df[df.apply(check_image_exists, axis=1)]\n\n\n# Dividir en train/test (ajusta según necesites)\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Crear datasets HuggingFace\ndef create_dataset(dataframe):\n    dataset = Dataset.from_dict({\n        'image': [Image.open(os.path.join(images_dir, fname)).convert('RGB') for fname in dataframe['filename']],\n        'text': list(dataframe['label'])\n    })\n    return dataset\n\ndataset = DatasetDict({\n    'train': create_dataset(train_df),\n    'test': create_dataset(test_df)\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T18:49:19.656702Z","iopub.execute_input":"2025-07-17T18:49:19.657062Z","iopub.status.idle":"2025-07-17T18:49:20.065624Z","shell.execute_reply.started":"2025-07-17T18:49:19.657043Z","shell.execute_reply":"2025-07-17T18:49:20.064818Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from transformers import TrOCRProcessor\n\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\ndef prepare_examples(batch):\n    images = batch['image']\n    texts = batch['text']\n    \n    # Procesar imágenes\n    pixel_values = processor(images, return_tensors=\"pt\").pixel_values\n    \n    # Procesar textos\n    labels = processor.tokenizer(texts, padding=\"max_length\", max_length=64).input_ids\n    \n    # Reemplazar padding token id por -100 para ignorar en loss\n    labels = [[label if label != processor.tokenizer.pad_token_id else -100 for label in labels_example] for labels_example in labels]\n    \n    batch['pixel_values'] = pixel_values\n    batch['labels'] = labels\n    \n    return batch\n\n# Aplicar el preprocesamiento\ntrain_dataset = dataset['train'].map(prepare_examples, batched=True, batch_size=8)\neval_dataset = dataset['test'].map(prepare_examples, batched=True, batch_size=8)\n\n# Configurar formato para PyTorch\ntrain_dataset.set_format(type='torch', columns=['pixel_values', 'labels'])\neval_dataset.set_format(type='torch', columns=['pixel_values', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T18:49:22.419842Z","iopub.execute_input":"2025-07-17T18:49:22.420427Z","iopub.status.idle":"2025-07-17T18:49:24.139655Z","shell.execute_reply.started":"2025-07-17T18:49:22.420405Z","shell.execute_reply":"2025-07-17T18:49:24.138786Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fca2dd049c14c82822f6eb4272523a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79d8b45dce0f449497520d831957a177"}},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n\n# Configurar parámetros del modelo\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# Configuración para generación\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T18:49:29.961028Z","iopub.execute_input":"2025-07-17T18:49:29.961522Z","iopub.status.idle":"2025-07-17T18:49:30.435707Z","shell.execute_reply.started":"2025-07-17T18:49:29.961496Z","shell.execute_reply":"2025-07-17T18:49:30.435069Z"}},"outputs":[{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\nimport evaluate\n\ncer_metric = evaluate.load(\"cer\")\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n    \n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n    \n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n    # Agrega esto a tus compute_metrics\n    wer_metric = evaluate.load(\"wer\")\n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n    \n    return {\"cer\": cer, \"wer\": wer}\n    \ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./trocr-trained\",\n    per_device_train_batch_size=4,  # Aumentar si tu GPU lo permite\n    per_device_eval_batch_size=4,\n    num_train_epochs=10,  # Más épocas\n    fp16=True,\n    learning_rate=3e-5,  # Tasa de aprendizaje más baja\n    eval_strategy=\"steps\",\n    eval_steps=200,  # Evaluar con menos frecuencia\n    save_steps=500,\n    logging_steps=50,\n    warmup_steps=100,  # Añadir warmup\n    weight_decay=0.01,  # Regularización\n    save_total_limit=2,\n    predict_with_generate=True,\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=processor.feature_extractor,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:29:24.244173Z","iopub.execute_input":"2025-07-17T19:29:24.244909Z","iopub.status.idle":"2025-07-17T19:29:24.556907Z","shell.execute_reply.started":"2025-07-17T19:29:24.244882Z","shell.execute_reply":"2025-07-17T19:29:24.556037Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/trocr/processing_trocr.py:152: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n  warnings.warn(\n/tmp/ipykernel_36/2952902863.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Iniciar entrenamiento\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:29:29.243424Z","iopub.execute_input":"2025-07-17T19:29:29.243739Z","iopub.status.idle":"2025-07-17T19:30:35.492549Z","shell.execute_reply.started":"2025-07-17T19:29:29.243715Z","shell.execute_reply":"2025-07-17T19:30:35.491669Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 01:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=50, training_loss=0.23392311096191407, metrics={'train_runtime': 64.8758, 'train_samples_per_second': 6.166, 'train_steps_per_second': 0.771, 'total_flos': 3.539581802643456e+17, 'train_loss': 0.23392311096191407, 'epoch': 10.0})"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"# Guardar el modelo entrenado\ntrainer.save_model(\"trocr-trained-custom\")\nprocessor.save_pretrained(\"trocr-trained-custom\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:30:37.619599Z","iopub.execute_input":"2025-07-17T19:30:37.619977Z","iopub.status.idle":"2025-07-17T19:30:42.140202Z","shell.execute_reply.started":"2025-07-17T19:30:37.619955Z","shell.execute_reply":"2025-07-17T19:30:42.139609Z"}},"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":89},{"cell_type":"code","source":"from transformers import pipeline\n\nimages_dir = \"/kaggle/input/image-test/\"\n\n# Cargar el modelo entrenado\nocr = pipeline(\"image-to-text\", model=\"./trocr-trained-custom\")\n\n# Probar con una imagen de test\ntest_image = Image.open(os.path.join(images_dir, \"image_test_3.png\")).convert(\"RGB\")\nprediction = ocr(test_image)\nprint(f\"Predicción: {prediction[0]['generated_text']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:32:49.140536Z","iopub.execute_input":"2025-07-17T19:32:49.141269Z","iopub.status.idle":"2025-07-17T19:32:50.562415Z","shell.execute_reply.started":"2025-07-17T19:32:49.141247Z","shell.execute_reply":"2025-07-17T19:32:50.561679Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Predicción: Bla, Gili Vide\n","output_type":"stream"}],"execution_count":93}]}
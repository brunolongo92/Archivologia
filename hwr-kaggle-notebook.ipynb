{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12492646,"sourceType":"datasetVersion","datasetId":7883632},{"sourceId":12500893,"sourceType":"datasetVersion","datasetId":7888566}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom datasets import DatasetDict, Dataset\nfrom PIL import Image\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments, pipeline\nimport evaluate\nimport gc\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:20:19.959769Z","iopub.execute_input":"2025-07-30T21:20:19.960524Z","iopub.status.idle":"2025-07-30T21:20:19.965591Z","shell.execute_reply.started":"2025-07-30T21:20:19.960487Z","shell.execute_reply":"2025-07-30T21:20:19.964740Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Cargar el CSV\ncsv_path = \"/kaggle/input/hwr-test/output_labels.csv\"\nimages_dir = \"/kaggle/input/hwr-test/output_lines\"\n\n# Leer el CSV\ndf = pd.read_csv(csv_path)\n\n# Verificar que las imágenes existen\ndef check_image_exists(row):\n    return os.path.exists(os.path.join(images_dir, row['filename']))\n\ndf = df[df.apply(check_image_exists, axis=1)]\n\n\n# Dividir en train/test (ajusta según necesites)\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Crear datasets HuggingFace\ndef create_dataset(dataframe):\n    dataset = Dataset.from_dict({\n        'image': [Image.open(os.path.join(images_dir, fname)).convert('RGB') for fname in dataframe['filename']],\n        'text': list(dataframe['label'])\n    })\n    return dataset\n\ndataset = DatasetDict({\n    'train': create_dataset(train_df),\n    'test': create_dataset(test_df)\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:20:28.334146Z","iopub.execute_input":"2025-07-30T21:20:28.334441Z","iopub.status.idle":"2025-07-30T21:20:28.696196Z","shell.execute_reply.started":"2025-07-30T21:20:28.334419Z","shell.execute_reply":"2025-07-30T21:20:28.695443Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\ndef prepare_examples(batch):\n    images = batch['image']\n    texts = batch['text']\n    \n    # Procesar imágenes\n    pixel_values = processor(images, return_tensors=\"pt\").pixel_values\n    \n    # Procesar textos\n    labels = processor.tokenizer(texts, padding=\"max_length\", max_length=64).input_ids\n    \n    # Reemplazar padding token id por -100 para ignorar en loss\n    labels = [[label if label != processor.tokenizer.pad_token_id else -100 for label in labels_example] for labels_example in labels]\n    \n    batch['pixel_values'] = pixel_values\n    batch['labels'] = labels\n    \n    return batch\n\n# Aplicar el preprocesamiento\ntrain_dataset = dataset['train'].map(prepare_examples, batched=True, batch_size=8)\neval_dataset = dataset['test'].map(prepare_examples, batched=True, batch_size=8)\n\n# Configurar formato para PyTorch\ntrain_dataset.set_format(type='torch', columns=['pixel_values', 'labels'])\neval_dataset.set_format(type='torch', columns=['pixel_values', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:20:33.234607Z","iopub.execute_input":"2025-07-30T21:20:33.234899Z","iopub.status.idle":"2025-07-30T21:20:35.518888Z","shell.execute_reply.started":"2025-07-30T21:20:33.234879Z","shell.execute_reply":"2025-07-30T21:20:35.518032Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0159a19aee7149f3b53542c2c4951aea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d6fd6282bf645a28bbcfaf3a84c8f6b"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n\n# Configurar parámetros del modelo\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# Configuración para generación\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:20:39.134559Z","iopub.execute_input":"2025-07-30T21:20:39.134846Z","iopub.status.idle":"2025-07-30T21:20:41.658849Z","shell.execute_reply.started":"2025-07-30T21:20:39.134826Z","shell.execute_reply":"2025-07-30T21:20:41.658092Z"}},"outputs":[{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"cer_metric = evaluate.load(\"cer\")\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n    \n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n    \n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n    # Agrega esto a tus compute_metrics\n    wer_metric = evaluate.load(\"wer\")\n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n    \n    return {\"cer\": cer, \"wer\": wer}\n    \ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./trocr-trained\",\n    per_device_train_batch_size=4,  # Aumentar si tu GPU lo permite\n    per_device_eval_batch_size=4,\n    num_train_epochs=10,  # Más épocas\n    fp16=True,\n    learning_rate=3e-5,  # Tasa de aprendizaje más baja\n    eval_strategy=\"steps\",\n    eval_steps=200,  # Evaluar con menos frecuencia\n    save_steps=500,\n    logging_steps=50,\n    warmup_steps=100,  # Añadir warmup\n    weight_decay=0.01,  # Regularización\n    save_total_limit=2,\n    predict_with_generate=True,\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=processor.feature_extractor,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:20:44.144734Z","iopub.execute_input":"2025-07-30T21:20:44.145015Z","iopub.status.idle":"2025-07-30T21:20:45.457196Z","shell.execute_reply.started":"2025-07-30T21:20:44.144994Z","shell.execute_reply":"2025-07-30T21:20:45.456402Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/trocr/processing_trocr.py:152: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n  warnings.warn(\n/tmp/ipykernel_267/1178572990.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\n# Iniciar entrenamiento\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:20:51.342630Z","iopub.execute_input":"2025-07-30T21:20:51.343320Z","iopub.status.idle":"2025-07-30T21:21:48.835419Z","shell.execute_reply.started":"2025-07-30T21:20:51.343286Z","shell.execute_reply":"2025-07-30T21:21:48.834525Z"}},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:54, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=50, training_loss=2.497156677246094, metrics={'train_runtime': 56.6357, 'train_samples_per_second': 7.063, 'train_steps_per_second': 0.883, 'total_flos': 3.539581802643456e+17, 'train_loss': 2.497156677246094, 'epoch': 10.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Guardar el modelo entrenado\ntrainer.save_model(\"trocr-trained-custom\")\nprocessor.save_pretrained(\"trocr-trained-custom\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:21:58.290574Z","iopub.execute_input":"2025-07-30T21:21:58.290895Z","iopub.status.idle":"2025-07-30T21:22:01.807142Z","shell.execute_reply.started":"2025-07-30T21:21:58.290872Z","shell.execute_reply":"2025-07-30T21:22:01.806356Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"images_dir = \"/kaggle/input/image-test/\"\n\n# Cargar el modelo entrenado\nocr = pipeline(\"image-to-text\", model=\"./trocr-trained-custom\")\n\n# Probar con una imagen de test\ntest_image = Image.open(os.path.join(images_dir, \"test_image.png\")).convert(\"RGB\")\nprediction = ocr(test_image)\nprint(f\"Predicción: {prediction[0]['generated_text']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:23:07.442613Z","iopub.execute_input":"2025-07-30T21:23:07.443252Z","iopub.status.idle":"2025-07-30T21:23:08.734043Z","shell.execute_reply.started":"2025-07-30T21:23:07.443223Z","shell.execute_reply":"2025-07-30T21:23:08.733370Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Predicción: Enque Francisco\n","output_type":"stream"}],"execution_count":14}]}
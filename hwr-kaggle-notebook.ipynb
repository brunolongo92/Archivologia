{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12492646,"sourceType":"datasetVersion","datasetId":7883632},{"sourceId":12500893,"sourceType":"datasetVersion","datasetId":7888566}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom datasets import DatasetDict, Dataset\nfrom PIL import Image\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments, pipeline\nimport evaluate\nimport gc\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:20:19.959769Z","iopub.execute_input":"2025-07-30T21:20:19.960524Z","iopub.status.idle":"2025-07-30T21:20:19.965591Z","shell.execute_reply.started":"2025-07-30T21:20:19.960487Z","shell.execute_reply":"2025-07-30T21:20:19.964740Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Cargar el CSV\ncsv_path = \"/kaggle/input/hwr-test/output_labels.csv\"\nimages_dir = \"/kaggle/input/hwr-test/output_lines\"\n\n# Leer el CSV\ndf = pd.read_csv(csv_path)\n\n# Verificar que las imágenes existen\ndef check_image_exists(row):\n    return os.path.exists(os.path.join(images_dir, row['filename']))\n\ndf = df[df.apply(check_image_exists, axis=1)]\n\n\n# Dividir en train/test (ajusta según necesites)\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Crear datasets HuggingFace\ndef create_dataset(dataframe):\n    dataset = Dataset.from_dict({\n        'image': [Image.open(os.path.join(images_dir, fname)).convert('RGB') for fname in dataframe['filename']],\n        'text': list(dataframe['label'])\n    })\n    return dataset\n\ndataset = DatasetDict({\n    'train': create_dataset(train_df),\n    'test': create_dataset(test_df)\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:20:28.334146Z","iopub.execute_input":"2025-07-30T21:20:28.334441Z","iopub.status.idle":"2025-07-30T21:20:28.696196Z","shell.execute_reply.started":"2025-07-30T21:20:28.334419Z","shell.execute_reply":"2025-07-30T21:20:28.695443Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\ndef prepare_examples(batch):\n    images = batch['image']\n    texts = batch['text']\n    \n    # Procesar imágenes\n    pixel_values = processor(images, return_tensors=\"pt\").pixel_values\n    \n    # Procesar textos\n    labels = processor.tokenizer(texts, padding=\"max_length\", max_length=64).input_ids\n    \n    # Reemplazar padding token id por -100 para ignorar en loss\n    labels = [[label if label != processor.tokenizer.pad_token_id else -100 for label in labels_example] for labels_example in labels]\n    \n    batch['pixel_values'] = pixel_values\n    batch['labels'] = labels\n    \n    return batch\n\n# Aplicar el preprocesamiento\ntrain_dataset = dataset['train'].map(prepare_examples, batched=True, batch_size=8)\neval_dataset = dataset['test'].map(prepare_examples, batched=True, batch_size=8)\n\n# Configurar formato para PyTorch\ntrain_dataset.set_format(type='torch', columns=['pixel_values', 'labels'])\neval_dataset.set_format(type='torch', columns=['pixel_values', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:20:33.234607Z","iopub.execute_input":"2025-07-30T21:20:33.234899Z","iopub.status.idle":"2025-07-30T21:20:35.518888Z","shell.execute_reply.started":"2025-07-30T21:20:33.234879Z","shell.execute_reply":"2025-07-30T21:20:35.518032Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0159a19aee7149f3b53542c2c4951aea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d6fd6282bf645a28bbcfaf3a84c8f6b"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n\n# Configurar parámetros del modelo\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# Configuración para generación\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T21:20:39.134559Z","iopub.execute_input":"2025-07-30T21:20:39.134846Z","iopub.status.idle":"2025-07-30T21:20:41.658849Z","shell.execute_reply.started":"2025-07-30T21:20:39.134826Z","shell.execute_reply":"2025-07-30T21:20:41.658092Z"}},"outputs":[{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"cer_metric = evaluate.load('cer')\n \ndef compute_cer(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n \n \n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n \n \n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n \n \n    return {\"cer\": cer}\n    \ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./trocr-trained\",\n    per_device_train_batch_size=4,  # Aumentar si tu GPU lo permite\n    per_device_eval_batch_size=4,\n    num_train_epochs=10,  # Más épocas\n    fp16=True,\n    learning_rate=3e-5,  # Tasa de aprendizaje más baja\n    eval_strategy=\"epoch\",\n    logging_strategy='epoch',\n    save_strategy='epoch',\n    eval_steps=200,  # Evaluar con menos frecuencia\n    save_steps=500,\n    logging_steps=50,\n    warmup_steps=100,  # Añadir warmup\n    weight_decay=0.01,  # Regularización\n    save_total_limit=2,\n    predict_with_generate=True,\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=processor.feature_extractor,\n    args=training_args,\n    compute_metrics=compute_cer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T22:28:07.488880Z","iopub.execute_input":"2025-07-30T22:28:07.489723Z","iopub.status.idle":"2025-07-30T22:28:08.081690Z","shell.execute_reply.started":"2025-07-30T22:28:07.489688Z","shell.execute_reply":"2025-07-30T22:28:08.080831Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/trocr/processing_trocr.py:152: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n  warnings.warn(\n/tmp/ipykernel_267/2644116373.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nres = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T22:28:19.759620Z","iopub.execute_input":"2025-07-30T22:28:19.759901Z","iopub.status.idle":"2025-07-30T22:31:31.157245Z","shell.execute_reply.started":"2025-07-30T22:28:19.759878Z","shell.execute_reply":"2025-07-30T22:31:31.156250Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 03:09, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Cer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.166100</td>\n      <td>6.598489</td>\n      <td>0.545946</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.161400</td>\n      <td>6.523168</td>\n      <td>0.551351</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.117600</td>\n      <td>6.459909</td>\n      <td>0.589189</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.109900</td>\n      <td>6.450336</td>\n      <td>0.551351</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.082400</td>\n      <td>6.468628</td>\n      <td>0.567568</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.072800</td>\n      <td>6.589461</td>\n      <td>0.529730</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.101400</td>\n      <td>6.674003</td>\n      <td>0.443243</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.087700</td>\n      <td>6.326686</td>\n      <td>0.524324</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.130100</td>\n      <td>6.309288</td>\n      <td>0.551351</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.164500</td>\n      <td>6.699563</td>\n      <td>0.475676</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"# Guardar el modelo entrenado\ntrainer.save_model(\"trocr-trained-custom\")\nprocessor.save_pretrained(\"trocr-trained-custom\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T22:31:44.517516Z","iopub.execute_input":"2025-07-30T22:31:44.518383Z","iopub.status.idle":"2025-07-30T22:31:48.999107Z","shell.execute_reply.started":"2025-07-30T22:31:44.518344Z","shell.execute_reply":"2025-07-30T22:31:48.998339Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"images_dir = \"/kaggle/input/image-test/\"\n\n# Cargar el modelo entrenado\nocr = pipeline(\"image-to-text\", model=\"./trocr-trained-custom\")\n\n# Probar con una imagen de test\ntest_image = Image.open(os.path.join(images_dir, \"test_image.png\")).convert(\"RGB\")\nprediction = ocr(test_image)\nprint(f\"Predicción: {prediction[0]['generated_text']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T22:32:24.240226Z","iopub.execute_input":"2025-07-30T22:32:24.240513Z","iopub.status.idle":"2025-07-30T22:32:25.454025Z","shell.execute_reply.started":"2025-07-30T22:32:24.240493Z","shell.execute_reply":"2025-07-30T22:32:25.453406Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Predicción: Cini Francisco\n","output_type":"stream"}],"execution_count":69}]}